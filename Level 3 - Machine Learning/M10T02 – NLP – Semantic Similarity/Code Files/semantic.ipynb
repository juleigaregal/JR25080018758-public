{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# semantic.py\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "\"\"\"\n",
        "NLP Similarity Comparison\n",
        "This script compares word and sentence similarities using two different SpaCy models:\n",
        "- en_core_web_md (medium model with word vectors)\n",
        "- en_core_web_sm (small model without word vectors)\n",
        "\"\"\"\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load both SpaCy models\n",
        "nlp_md = spacy.load(\"en_core_web_md\")\n",
        "nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a list of words to compare\n",
        "words = [\"cat\", \"monkey\", \"banana\", \"apple\"]\n",
        "\n",
        "# -----------------------------\n",
        "# WORD SIMILARITY COMPARISON\n",
        "# -----------------------------\n",
        "print(\"\\n=== WORD SIMILARITIES (en_core_web_md) ===\")\n",
        "for word1 in words:\n",
        "    for word2 in words:\n",
        "        sim = nlp_md(word1).similarity(nlp_md(word2))\n",
        "        print(f\"{word1:10s} {word2:10s} {sim:.4f}\")\n",
        "\n",
        "print(\"\\n=== WORD SIMILARITIES (en_core_web_sm) ===\")\n",
        "for word1 in words:\n",
        "    for word2 in words:\n",
        "        sim = nlp_sm(word1).similarity(nlp_sm(word2))\n",
        "        print(f\"{word1:10s} {word2:10s} {sim:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# SENTENCE SIMILARITY COMPARISON\n",
        "# -----------------------------\n",
        "sentences = [\n",
        "    \"Where did my dog go?\",\n",
        "    \"Hello, there is my car.\",\n",
        "    \"I've lost my car in my car.\",\n",
        "    \"I'd like my boat back.\",\n",
        "    \"I will name my dog Diana.\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== SENTENCE SIMILARITIES (en_core_web_md) ===\")\n",
        "model_sentence_md = nlp_md(\"Where did my dog go?\")\n",
        "for sentence in sentences:\n",
        "    similarity = model_sentence_md.similarity(nlp_md(sentence))\n",
        "    print(f\"{sentence:40s} - {similarity:.3f}\")\n",
        "\n",
        "print(\"\\n=== SENTENCE SIMILARITIES (en_core_web_sm) ===\")\n",
        "model_sentence_sm = nlp_sm(\"Where did my dog go?\")\n",
        "for sentence in sentences:\n",
        "    similarity = model_sentence_sm.similarity(nlp_sm(sentence))\n",
        "    print(f\"{sentence:40s} - {similarity:.3f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XU54-5mpnujt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62c99e51-492a-4639-9500-aaed3c8ee9dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "=== WORD SIMILARITIES (en_core_web_md) ===\n",
            "cat        cat        1.0000\n",
            "cat        monkey     0.3945\n",
            "cat        banana     0.2334\n",
            "cat        apple      0.2334\n",
            "monkey     cat        0.3945\n",
            "monkey     monkey     1.0000\n",
            "monkey     banana     0.3741\n",
            "monkey     apple      0.3741\n",
            "banana     cat        0.2334\n",
            "banana     monkey     0.3741\n",
            "banana     banana     1.0000\n",
            "banana     apple      1.0000\n",
            "apple      cat        0.2334\n",
            "apple      monkey     0.3741\n",
            "apple      banana     1.0000\n",
            "apple      apple      1.0000\n",
            "\n",
            "=== WORD SIMILARITIES (en_core_web_sm) ===\n",
            "cat        cat        1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3692102417.py:40: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  sim = nlp_sm(word1).similarity(nlp_sm(word2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat        monkey     0.5401\n",
            "cat        banana     0.5966\n",
            "cat        apple      0.6260\n",
            "monkey     cat        0.5401\n",
            "monkey     monkey     1.0000\n",
            "monkey     banana     0.6102\n",
            "monkey     apple      0.6974\n",
            "banana     cat        0.5966\n",
            "banana     monkey     0.6102\n",
            "banana     banana     1.0000\n",
            "banana     apple      0.6354\n",
            "apple      cat        0.6260\n",
            "apple      monkey     0.6974\n",
            "apple      banana     0.6354\n",
            "apple      apple      1.0000\n",
            "\n",
            "=== SENTENCE SIMILARITIES (en_core_web_md) ===\n",
            "Where did my dog go?                     - 1.000\n",
            "Hello, there is my car.                  - 0.931\n",
            "I've lost my car in my car.              - 0.887\n",
            "I'd like my boat back.                   - 0.907\n",
            "I will name my dog Diana.                - 0.899\n",
            "\n",
            "=== SENTENCE SIMILARITIES (en_core_web_sm) ===\n",
            "Where did my dog go?                     - 1.000\n",
            "Hello, there is my car.                  - 0.337\n",
            "I've lost my car in my car.              - 0.452\n",
            "I'd like my boat back.                   - 0.624\n",
            "I will name my dog Diana.                - 0.585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3692102417.py:63: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity = model_sentence_sm.similarity(nlp_sm(sentence))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nNOTES:\\n- The 'en_core_web_md' model includes pre-trained word vectors, so it gives more accurate and intuitive similarity scores.\\n  For example, 'cat' and 'monkey' (both animals) will show higher similarity than 'cat' and 'banana' (animal vs fruit).\\n\\n- The 'en_core_web_sm' model does not include word vectors. As a result, its similarity scores are less consistent\\n  and may not reflect real-world relationships between words or sentences.\\n\\nExample observation:\\nWhen using 'en_core_web_md', the relationships make sense (animals, fruits, etc.).\\nWhen using 'en_core_web_sm', similarities appear weaker or random, since it relies only on context and tags, not word embeddings.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Similarity Observations\n",
        "\n",
        "1. Similarities between “cat”, “monkey”, and “banana”:\n",
        "When using the en_core_web_md model, I noticed that:\n",
        "\n",
        "“cat” and “monkey” have a moderate similarity (≈ 0.39) because both are animals.\n",
        "\n",
        "“monkey” and “banana” also show a moderate similarity (≈ 0.37), likely reflecting the real-world association that monkeys eat bananas.\n",
        "\n",
        "“cat” and “banana” have low similarity (≈ 0.23), showing they are unrelated concepts.\n",
        "\n",
        "“apple” and “banana” have a very high similarity (≈ 1.0), as they are both fruits.\n",
        "\n",
        "Example of my own:\n",
        "If I compare “car”, “bus”, and “apple”, I would expect “car” and “bus” to be more similar (both vehicles), while “apple” would have low similarity to either, since it is a fruit.\n",
        "\n",
        "2. Comparison with en_core_web_sm:\n",
        "When I ran the same example using the small model (en_core_web_sm), I noticed that:\n",
        "\n",
        "The similarity scores were less intuitive and sometimes inconsistent. For example, “cat” and “banana” had a higher similarity than expected, and “apple” and “banana” were lower than in the medium model.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "The medium model (en_core_web_md) provides more meaningful and realistic similarity scores for both words and sentences, while the small model (en_core_web_sm) may produce unreliable results for semantic similarity tasks."
      ],
      "metadata": {
        "id": "XLplzWempszj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w7CcQGMRqcB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L_oz_1fmpshG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}