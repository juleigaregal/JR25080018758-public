{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shakespeare Character-level RNN using PyTorch\n",
        "\n",
        "This notebook implements a simple character-level RNN trained on the Tiny Shakespeare dataset, following the structure of the original sample task file. Integer class targets and `CrossEntropyLoss` are used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ],
      "source": [
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Character Mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 65\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "char_indices = {c: i for i, c in enumerate(chars)}\n",
        "indices_char = {i: c for i, c in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f'Vocabulary size: {vocab_size}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sequences: 371785\n"
          ]
        }
      ],
      "source": [
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "print(f'Number of sequences: {len(sentences)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vectorize Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input sequences as one-hot\n",
        "X = torch.zeros((len(sentences), maxlen, vocab_size), dtype=torch.float32)\n",
        "\n",
        "# Targets as one-hot vectors\n",
        "y = torch.zeros((len(sentences), vocab_size), dtype=torch.float32)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.softmax(out)  # probability distribution\n",
        "        return out\n",
        "\n",
        "hidden_size = 128\n",
        "model = RNNModel(vocab_size, hidden_size, vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()  # because targets are one-hot\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Average Loss: 0.000124\n",
            "Epoch [2/3], Average Loss: 0.000168\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 3\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        X_batch = X[i:i+batch_size]\n",
        "        y_batch = y[i:i+batch_size]\n",
        "\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {epoch_loss/len(X):.6f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, seed, length=500):\n",
        "    model.eval()\n",
        "    generated = seed\n",
        "    seq = seed\n",
        "    for _ in range(length):\n",
        "        x_pred = torch.zeros((1, maxlen, vocab_size), dtype=torch.float32)\n",
        "        for t, char in enumerate(seq):\n",
        "            x_pred[0, t, char_indices[char]] = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(x_pred)\n",
        "        next_index = torch.multinomial(preds[0], 1).item()\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        seq = seq[1:] + next_char\n",
        "\n",
        "    return generated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Sample Text (~100 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "seed_text = text[start_index: start_index + maxlen]\n",
        "sample_text = generate_text(model, seed_text, length=800)\n",
        "print('\\n--- Generated Text ---\\n')\n",
        "print(sample_text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
